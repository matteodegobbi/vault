# Text preprocessing

- Tokenization
	* split by space
	* split punctuation from words
	* split numbers
	it can improve vocabulary coverage (cat. cat, cat; etc)
- Remove Capitalization (not used for generative tasks)
- Lemmatization (not used for generative tasks) the lemmas are in a external dictionary we already have
- Stemming (not used for generative tasks), we keep only the stem of the words but the stems are learned from the corpus 
- Removing stop-words (not used for generative tasks)
	prepositions, determiners

