#dl #ml 

# Teoria ML
[[Empirical risk minimization]]
[[Train-test-validation, model selection]]
[[Bias-Variance and Bias-Complexity trade-offs]]
[[Metrics]]
# Models ML
OLS/linear reg
percpetron
logistic regression

# Architectures (broad)
GAN
Siamese Nets with triplet/contrastive loss, dissimilarity space and metric
CNNs: [[CNN]]
AutoEncoders
Transformers
ViTs
RNN / LSTM / GRU
TCN con atrous convolution (forse skip)
# Specific layers
Convolutional and pooling layers
Batchnorm
Global average pooling
Instance Normalization 
Spectral Normalization 
Layer Normalization 
Dropout
Activations
# Losses and optimizers
Cross-Entropy
BCE
MSE
L1 L2 reg
Weight decay
SGD
lr annealing
Momentum
Adam
AdamW
Segmentation losses like Dice and Tversky index

# Theoretical concepts
Bias variance tradeoff
k-fold validation
perceptron
exploding vanishing gradient
Xavier init, Kaiming-He init
Autodifferentiation
Distribution shift
Ensembles 
Distillation
Data augmentation
Scaling laws
Label smoothing
Gradient checkpointing

---
# Specific Models:
Inception/GoogLeNet
ResNet
GPT
BeRT
ElMO??
R-CNN, DeepLab, YOLO
Neural Style Transfer


