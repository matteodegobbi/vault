#ml 
[Paper clarifying differences between the two decompositions](https://openreview.net/pdf?id=4TnFbv16hK#cite.geman1992neural)
# Bias-Variance
## Derivation

We have a training set of feature label pairs $x,y$ sampled from a joint distribution.

We assume that the data is generated by a function $f(x)$ such that $y=f(x)+\varepsilon$, where the noise $\varepsilon$ has zero mean (not a strong assumption we could always just shift f if this were not the case) and variance $\sigma^2$ 
We want to find a function $\hat f$ that minimizes population MSE (equivalent to expected risk with squared loss in ERM terms). 

We are omitting the dependence on the dataset for brevity but in principle we should include it as we are taking the expectation on the dataset.

Let us write the mean-squared error of our model:

$$
\begin{align}
\text{MSE}(x) &\triangleq \mathbb{E}\Big[\big(y - \hat{f}\!(x)\big)^2\Big]\\
&= \mathbb{E}\Big[\big(f(x) + \varepsilon - \hat{f}\!(x)\big)^2\Big] && \text{since } y \triangleq f(x) + \varepsilon\\
&= \mathbb{E}\Big[\big(f(x) - \hat{f}\!(x)\big)^2\Big] \, + \, 2 \ \mathbb{E}\Big[ \big(f(x) - \hat{f}\!(x)\big) \varepsilon \Big] \, + \, \mathbb{E}[\varepsilon^2]
\end{align}
$$

We can show that the second term of this equation is null:

$$
\begin{align}
\mathbb{E}\Big[ \big(f(x) - \hat{f}\!(x)\big) \varepsilon \Big] &= \mathbb{E} \big[ f(x) - \hat{f}\!(x) \big] \ \mathbb{E} \big[ \varepsilon \big] && \text{since } \varepsilon \text{ is independent from } x\\
&= 0 && \text{since } \mathbb{E} \big[ \varepsilon \big] = 0
\end{align}
$$

Moreover, the third term of this equation is nothing but $\sigma^2$, the variance of $\varepsilon$, this is true since its mean is 0 so $\mathbb{var}[\varepsilon] = E[\varepsilon^2]$.

Let us now expand the remaining term:

$$
\begin{align}
& \mathbb{E}\left[\left(f(x) - \hat{f}\!(x)\right)^2\right] \\[1ex]
&= \mathbb{E}\left[\left(f(x) - \mathbb{E}[\hat{f}\!(x)] + \mathbb{E}[\hat{f}\!(x)] - \hat{f}\!(x)\right)^2\right]\\[1ex]
& = {\color{Yellow} \mathbb{E}\left[ \left( f(x) - \mathbb{E}[\hat{f}\!(x)] \right)^2 \right]}
\, + \, \mathbb{E} \left[ \left( \mathbb{E}[\hat{f}\!(x)] - \hat{f}\!(x) \right)^2 \right] \\
&\quad\, + \, 2 \ {\color{Orange} \mathbb{E} \left[ \left( f(x) - \mathbb{E}[\hat{f}\!(x)] \right) \left( \mathbb{E}[\hat{f}\!(x)] - \hat{f}\!(x) \right) \right]}
\end{align}
$$ 

We show that:

$$
\begin{align}
{ \color{Yellow}\mathbb{E}\Big[ \big( f(x) - \mathbb{E} \big[ \hat{f}(x) \big] \big)^2 \Big]} &= \mathbb{E} \big[ f(x) ^2 \big] \, - \, 2 \ \mathbb{E} \Big[ f(x) \ \mathbb{E} \big[ \hat{f}(x) \big] \Big] \, + \, \mathbb{E} \Big[ \mathbb{E} \big[ \hat{f}(x) \big]^2 \Big]\\
&= f(x)^2 \, - \, 2 \ f(x) \ \mathbb{E} \big[ \hat{f}(x) \big] \, + \, \mathbb{E} \big[ \hat{f}(x) \big]^2\\
&= \Big( f(x)  - \mathbb{E} \big[ \hat{f}(x) \big] \Big)^2
\end{align}
$$

This last series of equalities comes from the fact that $f(x)$ is not a random variable, but a fixed, deterministic function of $x$. Therefore, $\mathbb{E}\left[f(x)\right] = f(x)$ Similarly <math>\mathbb{E}\left[ f(x)^2 \right] = f(x)^2</math>, and <math>\mathbb{E} \left[ f(x) \, \mathbb{E}[\hat{f}\!(x)] \right] = f(x) \, \mathbb{E} \left[ \mathbb{E}[\hat{f}\!(x)] \right] = f(x) \mathbb{E}[\hat{f}\!(x)]</math>. Using the same reasoning, we can expand the second term and show that it is null:

$$
\begin{align}
&{\color{Orange} \mathbb{E} \left[ \left( f(x) - \mathbb{E}[\hat{f}\!(x)] \right) \left( \mathbb{E}[\hat{f}\!(x)] - \hat{f}\!(x) \right) \right]} \\
&= \mathbb{E} \left[ f(x)\, \mathbb{E}[\hat{f}\!(x)] \, - \, f(x) \hat{f}\!(x) \, - \, \mathbb{E}[\hat{f}\!(x)]^2 + \mathbb{E}[\hat{f}\!(x)] \, \hat{f}\!(x) \right] \\
&= f(x) \, \mathbb{E}[\hat{f}\!(x)] \, - \, f(x) \, \mathbb{E}[\hat{f}\!(x)] \, - \, \mathbb{E}[\hat{f}\!(x)]^2 \, + \, \mathbb{E}[\hat{f}\!(x)]^2\\
&= 0
\end{align}
$$

Eventually, we plug our derivations back into the original equation, and identify each term:

$$
\begin{align}
\text{MSE}(x) &= \left( f(x) - \mathbb{E}[\hat{f}\!(x)] \right)^2 + \mathbb{E} \left[ \left( \mathbb{E}[\hat{f}\!(x)] - \hat{f}\!(x) \right)^2 \right] + \sigma^2\\
&= {Bias}\left[\hat{f}\!(x) \right]^2 + \, {Var} \left[ \hat{f}\!(x) \right] \, + \, \sigma^2
\end{align}
$$

Finally, the MSE loss function (or equivalent to negative log-likelihood in the case of Gaussian dist) is obtained by taking the expectation value over <math>x\sim P</math>:
$$
\text{MSE} = \mathbb{E}_x \left[ \text{MSE}(x) \right] = \mathbb{E}_x \left\{{Bias}_D\!\left[\hat{f}\!(x;D)\right]^2 + {Var}_D\left[\hat{f}\!(x;D)\right]\right\} + \sigma^2.
$$

---
## Explanation 
The bias is a systematic component, independent of any particular training sample, and commonly regarded as measuring the ‘strength’ of a
model (often, but not always, bigger capacity models have less bias). 
The variance measures the sensitivity of $\hat f$ to changes in the training sample, independent of the true label y, so if we have high variance if we resample the training set the performance of $\hat f$ may vary a lot. The noise is a constant, independent of any model parameters and is irreducible.

There is a perceived trade-off with these terms. As the size of the (un-regularised) model increases: bias tends to decrease, and variance tends to increase. However, the trade-off can be more complex (e.g. with over-parameterized models) and the exact dynamics are an open research issue.

---

# True risk/generalization error decomposition

We can take a different decomposition of the expected risk (or generalization error, true risk etc.) this time considering the hypothesis obtained under ERM with a training set sampled from the joint distribution.

I follow the formulation of Understanding ML book where generalization error is defined as the risk function as in this note: [[Empirical risk minimization]] (and not the generalization gap version which subtracts the error of optimal Bayes predictor).

- $\mathcal{H}$ be a hypothesis class

- $\mathcal{D}$ be a distribution over $\mathcal{X} \times \mathcal{Y}$

- $L_{\mathcal{D}}(h)$ be the **true risk**:  
$$
L_{\mathcal{D}}(h) = \mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell(h(x),y)]  
$$

- $h_S = \arg\min_{h\in\mathcal{H}} L_S(h)$ (ERM solution)

$$
\begin{align}
L_{\mathcal{D}}(h_S) &=\underbrace{\min_{h \in \mathcal{H}} L_{\mathcal{D}}(h)}_{\varepsilon_{\mathrm{app}}} + \underbrace{\left( L_{\mathcal{D}}(h_S) - \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) \right)}_{\varepsilon_{\mathrm{est}}} \\ 
\varepsilon_{\mathrm{app}}  
&\coloneqq 
\min_{h \in \mathcal{H}} L_{\mathcal{D}}(h)  
\\ 
\varepsilon_{\mathrm{est}}  
&\coloneqq 
L_{\mathcal{D}}(h_S) - \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h)  
\end{align}
$$

In some papers we consider an additional term, the optimization error. It occurs for example if the learning algorithm does not find the exact ERM hypothesis, for example in SGD this term decreases at every iteration but might not be 0.

The **approximation error** contains the error of the optimal Bayes predictor 
(the best we could do if we considered all possible functions and we knew the distribution, which is not 0 whenever there's some nondeterminism) plus some additional error due to the fact that our hypothesis class $\cal H$ restricts the space of possible hypotheses. Summarizing approximation error = Bayes opt error + error from restrictions of $\cal H$.

Using some measure of hypothesis class size, e.g. VC-dim we will have that smaller hypothesis classes will have larger approximation errors than bigger hypothesis classes, e.g. considering a linear model vs polynomial linear model will have higher approximation error. 
The approximation error does not depend on the number of samples m but only on the chosen $\cal H$.

The **estimation error** is the difference between the true risk of the ERM hypothesis and the approximation error. It contains the part of the error not due to approximation but due to the fact that the empirical risk $L_S$ (training set error) is only an estimate of the true risk $L_D$ (generalization error).
The estimation error depends on both the number of training samples m and on the complexity of the hypothesis class $\cal H$.

---

# The 2 trade-offs compared
Increasing the complexity of $\cal H$ will usually yield a smaller $\varepsilon_{\mathrm{app}}$
but a bigger $\varepsilon_{\mathrm{est}}$ (increases logarithmically with VC-dim)
this means that there's a trade-off since our objective is to minimize the sum of these two terms, the generalization error $L_D$, this observation together with the NFL theorem mean that to develop learning algorithms we need to use some form of prior knowledge of the problem at hand in order to be able to learn.

The trade-off can be visualized as follows:
![[Pasted image 20260226153437.png]]

The 2 trade-offs discussed: bias-variance and approximation-estimation are related but the terms are not the exact same, except in OLS where approx=bias and estimation=variance.

An in-depth analysis of the differences can be found at 
[Paper clarifying differences between the two decompositions](https://openreview.net/pdf?id=4TnFbv16hK#cite.geman1992neural), this paper uses the gap version of generalization error so some differences in notation are present.

This diagram summarizes the differences:
![[Pasted image 20260226153651.png]]


Also in [this](https://stats.stackexchange.com/questions/431896/what-is-the-relationship-between-estimation-error-approximation-error-bias-va) thread the answer by Gavin Brown is useful to understand that bias-variance is a flawed proxy for approx-estimation but it's useful because it can be estimated from data, while approx-estimation cannot as it would need the knowledge of the unknown data distribution to be computed.

To estimate the bias-variance trade-off we can use resampling strategies like multiple splitting of train-test datasets or bootstrapping.

---

