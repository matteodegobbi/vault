#ml 
In ML we have an unknown data generating distribution and an unknown function that labels the data. Given an hypothesis class we want to learn a function in that class that given data generated by the distribution labels the data and minimizes the generalization error (or risk function / true error)
![[Pasted image 20260123234123.png]]

This generalization error cannot be known, so instead we use ERM:
We minimize the empirical risk on a given training set of data sampled from the data generating distribution:
![[Pasted image 20260123234627.png]]

We have theoretical guarantees of ERM with PAC learnability, uniform convergence and VC-dimension theory.

---

