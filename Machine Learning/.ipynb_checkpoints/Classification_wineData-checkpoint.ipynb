{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Classification: Wine Dataset\n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your notebook (this is the way we will do it for evaluating your HWs!)\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "We will be working with a dataset on wines from the UCI machine learning repository\n",
    "(http://archive.ics.uci.edu/ml/datasets/Wine). It contains data for 178 instances. \n",
    "The dataset is the results of a chemical analysis of wines grown in the same region\n",
    "in Italy but derived from three different cultivars. The analysis determined the\n",
    "quantities of 13 constituents found in each of the three types of wines. \n",
    "\n",
    "### The features in the dataset are:\n",
    "\n",
    "- Alcohol\n",
    "- Malic acid\n",
    "- Ash\n",
    "- Alcalinity of ash\n",
    "- Magnesium\n",
    "- Total phenols\n",
    "- Flavanoids\n",
    "- Nonflavanoid phenols\n",
    "- Proanthocyanins\n",
    "- Color intensity\n",
    "- Hue\n",
    "- OD280/OD315 of diluted wines\n",
    "-Proline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import the sklearn library\n",
    "\n",
    "\n",
    "#let's print out the version of scikit-learn\n",
    "\n",
    "\n",
    "#this imports the datasets module, which has useful datasets\n",
    "\n",
    "\n",
    "# Load the dataset from scikit learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the description of the dataset from the scikit learn documentation: https://scikit-learn.org/0.23/modules/classes.html#module-sklearn.datasets\n",
    "\n",
    "(**Note**: we are considering the scikit-learn version that is installed in the labs Te and Ue, but there are more recent ones)\n",
    "\n",
    "Now let's understand a little bit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's print the data matrix\n",
    "\n",
    "\n",
    "#let's print the dimension of the data matrix\n",
    "\n",
    "\n",
    "#let's print the target (labels)\n",
    "\n",
    "\n",
    "#let's print the features names\n",
    "\n",
    "\n",
    "#let's print the targets names names\n",
    "\n",
    "\n",
    "#let's print the description of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify a bit the problem (and the presentation), we are going to classify class \"1\" vs the other two classes (0 and 2). We are going to relabel the other classes (0 and 2) as \"-1\".\n",
    "\n",
    "For convenience, let's save the instances (vectors of features) in matrix $\\mathbf{X}$ and the targets into a vector $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \n",
    "Y = \n",
    "\n",
    "#let's print out the matrix of instances and the vector of targets, just to make sure that everything looks ok\n",
    "print(\"Matrix of instances\")\n",
    "print(X)\n",
    "\n",
    "print(\"Vector of labels\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's relabel the labels for classes 0 and 2 as stated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's relabel classes 0 and 2 as -1\n",
    "\n",
    "\n",
    "        \n",
    "#let's print the new vector Y\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Split into Training and Testing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually learn the model, it is important that we perform two operations:\n",
    "1. split the data into a training set and a test set\n",
    "2. normalize the features\n",
    "\n",
    "**Note**: some of there operations can be done with scikit-learn functions, but we do them \"manually\" to get a better understanding of what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to split the data into training and testing. Let's say we keep 80% of the data for training and 20% for testing. How do we split the data?\n",
    "\n",
    "What about keeping the first 80% of the raws for training and the last 20% of rows for testing? Is it a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: ...\n",
    "\n",
    "**Note**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to import numpy\n",
    "\n",
    "\n",
    "# set the random seed to your ID number\n",
    "IDnumber = \n",
    "np.random.seed(IDnumber)\n",
    "\n",
    "#let's generate a permutation among the number of rows\n",
    "\n",
    "\n",
    "#let's print Y_perm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data and save into 2 new data matrices/vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now center and scale the data to have unit variance. This is an important step for the stability of the computation and for other reasons. We are going to use the standard scaler from scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the StandardScaler module\n",
    "\n",
    "\n",
    "# we first \"learn\" the scaler function using the training data\n",
    "\n",
    "\n",
    "# we then apply the scaling function to both training and test data, since we want to simulate what happens when we have data for training and we have future data\n",
    "\n",
    "\n",
    "#let's print the scaled version of X_traing_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Model ##\n",
    "\n",
    "We now need to decide which model/algorithm we are going to use for our classification task. There are several models available in scikit-learn: https://scikit-learn.org/0.23/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start from the simplest models, that is, linear models: https://scikit-learn.org/0.23/modules/classes.html#module-sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the best hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a loss function and then use Empirical Risk Minimization (ERM). \n",
    "\n",
    "What loss function does it make sense to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the actual algorithm? We are going to consider the **Perceptron** algorithm: https://scikit-learn.org/0.23/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "\n",
    "Let's load the corresponding module in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us the Perceptron algorithm as implemented in scikit-learn. It proceeds in iterations.\n",
    "\n",
    "The Perceptron has several parameters, some of which we will understand later on. An important one is $\\texttt{tol}$, that represents how much the training error should improve in one iteration for the algorithm to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's learn a model using Perceptron\n",
    "\n",
    "#we first define the classifier, fixing the random state for reproducibility\n",
    "\n",
    "\n",
    "#let's now learn the classifier (i.e., run the perceptron to fix the weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the model we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our method perform?\n",
    "\n",
    "We need to compute the training error of the hypothesis $h_S$ we learned from the training set $S$. There is no function in python to compute the training error $L_S(h_S)$. However, there is a function to compute the \\emph{score}, that for the 0-1 loss corresponds to $1 - L_S(h_S)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compute the training error\n",
    "\n",
    "\n",
    "#let's print the training error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't care about the training error... we are interested in the generalization error! How do we estimate it? Let's use some data that we did not use for training, that is what we called test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compute the test error\n",
    "\n",
    "\n",
    "#let's print the test error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of the amount of training data ##\n",
    "\n",
    "We will now try to understand the impact of the amount of data we have for training.\n",
    "\n",
    "To do this, we are going to train a model using a subset of the data with $10*i$ samples, for $i=1,2,3,\\dots,10$, and then compute the training error and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of samples, useful for later on\n",
    "m_total = X.shape[0]\n",
    "\n",
    "#two lists where to save the training error and the test error, useful for plotting\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "\n",
    "#let's define the learner we use in this part\n",
    "perceptron_class = Perceptron(random_state = IDnumber, tol=1e-3 )\n",
    "\n",
    "for i in range(1,10):\n",
    "    # we now repeat all the previous steps\n",
    "    # split into training and test\n",
    "\n",
    "    \n",
    "    #scale the data according to the training test, for both training and testing\n",
    "\n",
    "    \n",
    "    #let's now learn the classifier (i.e., run the perceptron to fix the weights\n",
    "\n",
    "print(train_errors)\n",
    "print(test_errors)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the training and test error as a function of the training dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following is to have the plots appearing inline\n",
    "%matplotlib inline\n",
    "\n",
    "#import the pyplot module from matplotlib for plotting (functions are similar to matlab)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis=range(10,100,10)\n",
    "plt.plot(x_axis,train_errors,'x:')\n",
    "plt.plot(x_axis,test_errors,'o:')\n",
    "plt.legend([\"Training error\",\"Test error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Impact of initial conditions by the perceptron \n",
    "\n",
    "Note that the solution found by the Perceptron algorithm depends on the initial condition. Let's learn a model with a different random seed for the Perceptron and see how different the model is from the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's learn a new model using Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of normalization\n",
    "\n",
    "Let's try to understand what the impact of scaling data is. Let's learn a model without without normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's learn a new model using Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of number of iterations\n",
    "\n",
    "Let's write the code that performs one iteration at the time, and let' compute the training error after iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
