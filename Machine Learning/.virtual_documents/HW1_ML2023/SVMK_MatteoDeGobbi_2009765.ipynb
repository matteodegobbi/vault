








#put here your ``numero di matricola''
numero_di_matricola = 2009765# COMPLETE





#import all packages needed
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn import svm
from sklearn import model_selection
from sklearn import linear_model





#load the data - do not change the path below!
df = pd.read_csv('kc_house_data.csv', sep = ',')

#remove the data samples with missing values (NaN)
df = df.dropna() 

Data = df.values
m = Data.shape[0]
Y = Data[:m,2]
X = Data[:m,3:]

print("Total number of samples:",m)








# COMPLETE
m_train = int(0.6*m)
m_val = int((m-m_train)/2.0)
m_test = m - m_train - m_val 

Xtrain_val,Xtest,Ytrain_val,Ytest = train_test_split(X,Y,test_size=m_test/m, random_state=numero_di_matricola)
Xtrain, Xval, Ytrain, Yval = train_test_split(Xtrain_val,Ytrain_val,test_size=m_val/(m_val+m_train),random_state=numero_di_matricola)
#######
print("Training size: ", Xtrain.shape[0])
print("Validation size: ", Xval.shape[0])
print("Training and validation size",Xtrain_val.shape[0])
print("Test size",Xtest.shape[0])





# COMPLETE
scaler = preprocessing.StandardScaler().fit(Xtrain)
Xtrain_scaled= scaler.transform(Xtrain)
Xval_scaled = scaler.transform(Xval)
Xtrain_val_scaled = scaler.transform(Xtrain_val)
Xtest_scaled = scaler.transform(Xtest)
#############








print("\nLinear SVM")
# COMPLETE
from sklearn.model_selection import GridSearchCV

params5fold = {'C':[0.1, 1, 10, 100, 1000]}
lin_svm = svm.SVR(kernel='linear',epsilon=1000)
lin_svm5fold = GridSearchCV(lin_svm,param_grid=params5fold,cv=5)
lin_svm5fold.fit(Xtrain_scaled,Ytrain)
####################
print("Best value for hyperparameters: ", lin_svm5fold.best_params_)# COMPLETE)





# COMPLETE
best_lin_svm = svm.SVR(kernel='linear',epsilon=1000,C=lin_svm5fold.best_params_['C'])
best_lin_svm.fit(Xtrain_scaled,Ytrain)
#################
print("Training score: ",best_lin_svm.score(Xtrain_scaled,Ytrain)) # COMPLETE)





print("\nPoly SVM")
# COMPLETE
params5fold = {'C':[0.1, 1, 10, 100, 1000],'degree':[2,3,4]}
poly_svm = svm.SVR(kernel='poly',epsilon=1000)
poly_svm5fold = GridSearchCV(poly_svm,param_grid=params5fold,cv=5)
poly_svm5fold.fit(Xtrain_scaled,Ytrain)

#########
print("Best value for hyperparameters: ",poly_svm5fold.best_params_ ) # COMPLETE)





# COMPLETE
best_poly_svm = svm.SVR(kernel='poly',epsilon=1000,
                        C=poly_svm5fold.best_params_['C'],
                        degree=poly_svm5fold.best_params_['degree'])
best_poly_svm.fit(Xtrain_scaled,Ytrain)
################
print("Training score: ",best_poly_svm.score(Xtrain_scaled,Ytrain)) # COMPLETE)





print("\nRBF SVM")
# COMPLETE
params5fold = {'C':[0.1, 1, 10, 100, 1000],'gamma':[0.01]}
rbf_svm = svm.SVR(kernel='rbf',epsilon=1000)
rbf_svm5fold = GridSearchCV(rbf_svm,param_grid=params5fold,cv=5)
rbf_svm5fold.fit(Xtrain_scaled,Ytrain)
############################33
print("Best value for hyperparameters: ",rbf_svm5fold.best_params_ ) # COMPLETE)





#COMPLETE 
best_rbf_svm = svm.SVR(kernel='rbf',epsilon=1000,C=rbf_svm5fold.best_params_['C'],gamma=rbf_svm5fold.best_params_['gamma'])
best_rbf_svm.fit(Xtrain_scaled,Ytrain)
################
print("Training score: ",best_rbf_svm.score(Xtrain_scaled,Ytrain)) # COMPLETE)





print("\nSigmoid SVM")
# COMPLETE
params5fold = {'C':[0.1, 1, 10, 100, 1000],'gamma':[0.01],'coef0':[0,1]}
sig_svm = svm.SVR(kernel='sigmoid',epsilon=1000)
sig_svm5fold = GridSearchCV(sig_svm,param_grid=params5fold,cv=5)
sig_svm5fold.fit(Xtrain_scaled,Ytrain)
########3
print("Best value for hyperparameters: ",sig_svm5fold.best_params_ ) # COMPLETE)





# COMPLETE
best_sig_svm = svm.SVR(kernel='sigmoid',epsilon=1000,
                       C=sig_svm5fold.best_params_['C'],
                       gamma=sig_svm5fold.best_params_['gamma'],
                       coef0=sig_svm5fold.best_params_['coef0'])
best_sig_svm.fit(Xtrain_scaled,Ytrain)
################
print("Training score: ",best_sig_svm.score(Xtrain_scaled,Ytrain)) # COMPLETE)





print("\nVALIDATION TO CHOOSE SVM KERNEL")

# COMPLETE - INCLUDING THE FIRST 4 PRINT FROM THE LIST ABOVE
lin_val_score = best_lin_svm.score(Xval_scaled,Yval)
poly_val_score = best_poly_svm.score(Xval_scaled,Yval)
rbf_val_score = best_rbf_svm.score(Xval_scaled,Yval)
sig_val_score = best_sig_svm.score(Xval_scaled,Yval)
validation_scores = {"linear":lin_val_score,
                     "poly":poly_val_score,
                     "rbf":rbf_val_score,
                     "sigmoid":sig_val_score
                    }
print(f"Linear validation score: {lin_val_score}")
print(f"Poly validation score: {poly_val_score}")
print(f"rbf validation score:{ rbf_val_score}")
print(f"Sigmoid validation score: {sig_val_score}")
best_validation_kernel = max(validation_scores,key=validation_scores.get)
############################################################
print("Best kernel: ", best_validation_kernel)# COMPLETE)
print("Validation score of best kernel: ", validation_scores[best_validation_kernel])# COMPLETE)





print("\nTRAINING SCORE BEST MODEL")
# COMPLETE
if best_validation_kernel == 'linear':
    final_model = svm.SVR(kernel='linear',epsilon=1000,
                          C=lin_svm5fold.best_params_['C'])
elif best_validation_kernel == 'poly':
    final_model = svm.SVR(kernel='poly',epsilon=1000,
                            C=poly_svm5fold.best_params_['C'],
                            degree=poly_svm5fold.best_params_['degree'])
elif best_validation_kernel == 'rbf':
    final_model = svm.SVR(kernel='rbf',epsilon=1000,
                          C=rbf_svm5fold.best_params_['C'],
                          gamma=rbf_svm5fold.best_params_['gamma'])
else:
    final_model= svm.SVR(kernel='sigmoid',epsilon=1000,
                           C=sig_svm5fold.best_params_['C'],
                           gamma=sig_svm5fold.best_params_['gamma'],
                           coef0=sig_svm5fold.best_params_['coef0'])
final_model.fit(Xtrain_val_scaled,Ytrain_val)
#############################################
print("Score of the best model on the data used to learn it: ", 
      final_model.score(Xtrain_val_scaled,Ytrain_val)) # COMPLETE)





print("\nGENERALIZATION SCORE BEST MODEL")

print("Estimate of the generalization score for best SVM model: ",
      final_model.score(Xtest_scaled,Ytest))# COMPLETE)





print("\nANSWER")

#note that you may have to invert the order of the following 2 lines, print the smallest 1 first. THE VALUES HERE SHOULD BE HARD CODED!
print("Generalization score: ",0.6190151303458201) # COMPLETE )
print("Training score: ",0.6431316190704458 )# COMPLETE )

#the following is a string with you anwer
motivation = '''yes\nThe generalization score is lower than the training score.
This is in agreement with the theory because it admits
a generalization error bigger than the training error.
This is not always the case, it could happen that a model performs better on unseen data than on the training set.
However in practice it's common to have a lower generalization score than training score because of overfitting.
Also we are only estimating the generalization error through the test set
and we can't know it's real value
'''

print(motivation)





print("\nLR MODEL")
# COMPLETE
LR = linear_model.LinearRegression()
LR.fit(Xtrain_val_scaled,Ytrain_val)
######
print("Score of LR model on data used to learng it: ",
      LR.score(Xtrain_val_scaled,Ytrain_val) ) # COMPLETE)
print("Generalization score of LR model: ",
      LR.score(Xtest_scaled,Ytest))# COMPLETE)





















